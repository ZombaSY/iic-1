Created Experiment : CCzeqUEKdsAc
A:\Users\SSY\Documents\GitHub\Invariant-Information-Clustering\train_classifier.py
{'code': 0, 'azure_blob_id': 234}
DefaultEndpointsProtocol=https;AccountName=digocloud;AccountKey=gajF2Kr5ybF1knSwG+B7vPUpMUFudoiPE583IIlOcxeWdX/rtnd5gescD1huOJ3+aI18fEAucDVxMdFnkGcX0g==;EndpointSuffix=core.windows.net
Uploading... HumanSegmantation\CCzeqUEKdsAc\source\train_classifier.py
Upload Succeed!!
Login Complete, Welcome tjxhwl@gmail.com
Classifier(
  (encoder): Sequential(
    (0): ReplicationPad2d((1, 1, 1, 1))
    (1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU(inplace=True)
    (4): ReplicationPad2d((1, 1, 1, 1))
    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): ReLU(inplace=True)
    (8): ReplicationPad2d((1, 1, 1, 1))
    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ReplicationPad2d((1, 1, 1, 1))
    (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))
    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): ReplicationPad2d((1, 1, 1, 1))
    (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): ReplicationPad2d((1, 1, 1, 1))
    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))
    (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): ReLU(inplace=True)
    (25): ReplicationPad2d((1, 1, 1, 1))
    (26): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))
    (27): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): ReLU(inplace=True)
    (29): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (30): ReplicationPad2d((1, 1, 1, 1))
    (31): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))
    (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (33): ReLU(inplace=True)
    (34): ReplicationPad2d((1, 1, 1, 1))
    (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))
    (36): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (37): ReLU(inplace=True)
    (38): ReplicationPad2d((1, 1, 1, 1))
    (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))
    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (41): ReLU(inplace=True)
    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (43): ReplicationPad2d((1, 1, 1, 1))
    (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))
    (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): ReLU(inplace=True)
    (47): ReplicationPad2d((1, 1, 1, 1))
    (48): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))
    (49): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): ReLU(inplace=True)
    (51): ReplicationPad2d((1, 1, 1, 1))
    (52): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))
    (53): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): ReLU(inplace=True)
    (55): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (output_block): Vectorizer(
    (classifier): Linear(in_features=512, out_features=10, bias=True)
  )
)
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
  0%|                                                                                                      | 0/117 [00:00<?, ?it/s]A:\Users\SSY\anaconda3\envs\pytorch-dev-latest\lib\site-packages\torch\nn\functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.
  warnings.warn("Default grid_sample and affine_grid behavior will be changed "
A:\Users\SSY\anaconda3\envs\pytorch-dev-latest\lib\site-packages\torch\nn\functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.
  warnings.warn("Default grid_sample and affine_grid behavior will be changed "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch: 0 Adam LR: 0.0001 train Loss: 0.005726201460 Accuracy 11.5234% 59/512 of 60000:   0%|               | 0/117 [00:03<?, ?it/s]A:\Users\SSY\anaconda3\envs\pytorch-dev-latest\lib\site-packages\apex-0.1-py3.7.egg\apex\amp\wrap.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return orig_fn(*new_args, **kwargs)
Epoch: 0 Adam LR: 0.0001 train Loss: 0.005726201460 Accuracy 11.5234% 59/512 of 60000:   1%|       | 1/117 [00:03<06:19,  3.27s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch: 0 Adam LR: 0.0001 train Loss: 0.005996665452 Accuracy 11.2305% 115/1024 of 60000:   1%|     | 1/117 [00:04<06:19,  3.27s/it]Epoch: 0 Adam LR: 0.0001 train Loss: 0.005996665452 Accuracy 11.2305% 115/1024 of 60000:   2%|     | 2/117 [00:04<05:03,  2.64s/it]Epoch: 0 Adam LR: 0.0001 train Loss: 0.006518148196 Accuracy 10.6120% 163/1536 of 60000:   2%|     | 2/117 [00:05<05:03,  2.64s/it]Epoch: 0 Adam LR: 0.0001 train Loss: 0.006518148196 Accuracy 10.6120% 163/1536 of 60000:   3%|▏    | 3/117 [00:05<04:15,  2.24s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.012329384452 Accuracy 10.4980% 215/2048 of 60000:   3%|    | 3/117 [00:06<04:15,  2.24s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.012329384452 Accuracy 10.4980% 215/2048 of 60000:   3%|▏   | 4/117 [00:07<03:43,  1.98s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.064140665345 Accuracy 10.4297% 267/2560 of 60000:   3%|▏   | 4/117 [00:08<03:43,  1.98s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.064140665345 Accuracy 10.4297% 267/2560 of 60000:   4%|▏   | 5/117 [00:08<03:20,  1.79s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.108365321998 Accuracy 10.4492% 321/3072 of 60000:   4%|▏   | 5/117 [00:09<03:20,  1.79s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.108365321998 Accuracy 10.4492% 321/3072 of 60000:   5%|▏   | 6/117 [00:09<03:03,  1.66s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch: 0 Adam LR: 0.0001 train Loss: -0.153921646598 Accuracy 10.1283% 363/3584 of 60000:   5%|▏   | 6/117 [00:10<03:03,  1.66s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.153921646598 Accuracy 10.1283% 363/3584 of 60000:   6%|▏   | 7/117 [00:11<02:48,  1.53s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.190076049301 Accuracy 10.2539% 420/4096 of 60000:   6%|▏   | 7/117 [00:12<02:48,  1.53s/it]Epoch: 0 Adam LR: 0.0001 train Loss: -0.190076049301 Accuracy 10.2539% 420/4096 of 60000:   7%|▎   | 8/117 [00:12<02:41,  1.48s/it]